diff --git a/keras/src/dtype_policies/dtype_policy.py b/keras/src/dtype_policies/dtype_policy.py
index 975b6a293..270ade7cd 100644
--- a/keras/src/dtype_policies/dtype_policy.py
+++ b/keras/src/dtype_policies/dtype_policy.py
@@ -350,7 +350,11 @@ def _get_quantized_dtype_policy_by_str(policy):
             f"Received: policy={policy}"
         )
     mode, source_name = split_name
-    if policy.startswith("int8") or policy.startswith("int4"):
+    if (
+        policy.startswith("int8")
+        or policy.startswith("int4")
+        or policy.startswith("gptq")
+    ):
         return QuantizedDTypePolicy(mode, source_name)
     elif policy.startswith("float8"):
         return QuantizedFloat8DTypePolicy(mode, source_name)
diff --git a/keras/src/layers/core/dense.py b/keras/src/layers/core/dense.py
index 317d6bbd2..9326114dd 100644
--- a/keras/src/layers/core/dense.py
+++ b/keras/src/layers/core/dense.py
@@ -1,3 +1,4 @@
+from math import ceil
 import ml_dtypes
 
 from keras.src import activations
@@ -9,6 +10,7 @@ from keras.src import regularizers
 from keras.src.api_export import keras_export
 from keras.src.layers.input_spec import InputSpec
 from keras.src.layers.layer import Layer
+from keras.src.quantizers.quantizers import dequantize_with_sz_map, dequantize_with_zero_point
 
 
 @keras_export("keras.layers.Dense")
@@ -109,7 +111,7 @@ class Dense(Layer):
         kernel_shape = (input_shape[-1], self.units)
         if self.quantization_mode:
             self.quantized_build(kernel_shape, mode=self.quantization_mode)
-        if self.quantization_mode not in ("int8", "int4"):
+        if self.quantization_mode not in ("int8", "int4", "gptq"):
             # If the layer is quantized to int8 or int4, `self._kernel` will be
             # added in `self._int8_build` or `_int4_build`. Therefore, we skip
             # it here.
@@ -325,16 +327,19 @@ class Dense(Layer):
 
     # Quantization-related (int8 and float8) methods
 
-    def quantized_build(self, kernel_shape, mode):
+    def quantized_build(self, kernel_shape, mode, config):
         if mode == "int8":
             self._int8_build(kernel_shape)
         elif mode == "int4":
             self._int4_build(kernel_shape)
         elif mode == "float8":
             self._float8_build()
+        elif mode == "gptq":
+            self._gptq_build(kernel_shape, config)
         else:
             raise self._quantization_mode_error(mode)
-        self._is_quantized = True
+        if mode != "gptq":
+            self._is_quantized = True
 
     def _int8_build(self, kernel_shape):
         self.inputs_quantizer = quantizers.AbsMaxQuantizer(axis=-1)
@@ -352,6 +357,69 @@ class Dense(Layer):
             trainable=False,
         )
 
+    def _gptq_build(self, kernel_shape, config):
+        self.qptq_initialized = False
+        # self.inputs_quantizer = quantizers.AbsMaxQuantizer(axis=-1)
+        self.kernel_shape = kernel_shape
+        self.quantized_kernel = self.add_weight(
+            name="kernel",
+            shape=kernel_shape,
+            initializer="zeros",
+            dtype="int32",
+            trainable=False,
+        )
+
+        if config.group_size == -1:
+            n_groups = 1
+        else:
+            n_groups = ceil(self.kernel_shape[0] / config.group_size)
+        self.kernel_scale = self.add_weight(
+            name="kernel_scale",
+            shape=(self.units, n_groups),
+            initializer="ones",
+            dtype="float32",
+            trainable=False,
+        )
+        self.kernel_zero = self.add_weight(
+            name="kernel_zero",
+            shape=(self.units, n_groups),
+            initializer="zeros",
+            dtype="float32",
+            trainable=False,
+        )
+        self.g_idx = self.add_weight(
+            name="g_idx",
+            shape=(self.kernel_shape[0],),
+            initializer="zeros",
+            dtype="int32",
+            trainable=False,
+        )
+
+    def _gptq_call(self, inputs, training=False):
+        if not self._is_quantized:
+            return self.call(inputs, training=training)
+        # Cast inputs to float32 for arithmetic
+        # x = ops.cast(inputs, "float32")
+
+        # Dequantize weights on-the-fly:
+        # W = (q - zp) * s   with all tensors broadcast-aligned
+        # q = ops.cast(self.quantized_kernel, "float32")
+        W = (
+            ops.transpose(
+                dequantize_with_sz_map(
+                    self.quantized_kernel,
+                    self.kernel_scale,
+                    self.kernel_zero,
+                    self.g_idx,
+                )
+            ),
+        )
+
+        y = ops.matmul(inputs, W)  # [batch, out_features]
+        if getattr(self, "use_bias", False):
+            y = ops.add(y, self.bias)
+        return y
+
     def _int4_build(self, kernel_shape):
         """Build variables for int4 quantization.
 
@@ -617,7 +685,7 @@ class Dense(Layer):
             x = self.activation(x)
         return x
 
-    def quantize(self, mode, type_check=True):
+    def quantize(self, mode, type_check=True, config=None):
         # Prevent quantization of the subclasses
         if type_check and (type(self) is not Dense):
             raise self._not_implemented_error(self.quantize)
@@ -652,6 +720,8 @@ class Dense(Layer):
             # Assign packed values.
             self._kernel.assign(packed_kernel_value)
             self.kernel_scale.assign(kernel_scale)
+        elif mode == "gptq":
+            self.quantized_build(kernel_shape, mode, config)
         elif mode == "float8":
             self.quantized_build(kernel_shape, mode)
         else:
@@ -662,7 +732,11 @@ class Dense(Layer):
             from keras.src import dtype_policies  # local import to avoid cycle
 
             policy = dtype_policies.get(f"{mode}_from_{self.dtype_policy.name}")
+            if mode == "gptq":
+                self._is_quantized = True
             self.dtype_policy = policy
+            if mode == "gptq":
+                self._is_quantized = False
 
     def _get_kernel_with_merged_lora(self):
         """Returns the kernel with LoRA matrices merged, for serialization.
diff --git a/keras/src/layers/core/einsum_dense.py b/keras/src/layers/core/einsum_dense.py
index 39100486c..defe7ca6e 100644
--- a/keras/src/layers/core/einsum_dense.py
+++ b/keras/src/layers/core/einsum_dense.py
@@ -1,3 +1,4 @@
+from math import ceil
 import re
 import string
 
@@ -14,6 +15,7 @@ from keras.src import regularizers
 from keras.src.api_export import keras_export
 from keras.src.layers.input_spec import InputSpec
 from keras.src.layers.layer import Layer
+from keras.src.quantizers.quantizers import dequantize_with_sz_map, dequantize_with_zero_point
 
 
 @keras_export("keras.layers.EinsumDense")
@@ -167,7 +169,7 @@ class EinsumDense(Layer):
         # quantized to int8 or int4, because `quantized_build` has created the
         # appropriate kernel variable. For other modes (e.g., float8 or no
         # quantization), we still need the floating-point kernel.
-        if self.quantization_mode not in ("int8", "int4"):
+        if self.quantization_mode not in ("int8", "int4", "gptq"):
             # If the layer is quantized to int8, `self._kernel` will be added
             # in `self._int8_build`. Therefore, we skip it here.
             self._kernel = self.add_weight(
@@ -389,16 +391,19 @@ class EinsumDense(Layer):
 
     # Quantization-related (int8 and float8) methods
 
-    def quantized_build(self, kernel_shape, mode):
+    def quantized_build(self, kernel_shape, mode, config):
         if mode == "int8":
             self._int8_build(kernel_shape)
         elif mode == "int4":
             self._int4_build(kernel_shape)
         elif mode == "float8":
             self._float8_build()
+        elif mode == "gptq":
+            self._gptq_build(kernel_shape, config=config)
         else:
             raise self._quantization_mode_error(mode)
-        self._is_quantized = True
+        if mode != "gptq":
+            self._is_quantized = True
 
     def _int8_build(self, kernel_shape):
         self._set_quantization_info()
@@ -420,6 +425,107 @@ class EinsumDense(Layer):
             trainable=False,
         )
 
+    def _gptq_build(self, kernel_shape, config):
+        """
+        Allocate quantized kernel & params for EinsumDense.
+
+        Args:
+            kernel_shape: tuple/list; the layer's original kernel shape, e.g.
+                [in_features, out_features] or [in_features, heads, head_dim].
+            group_size: int; contiguous input-group size for quantization
+                (=-1 means per-output-channel with no grouping).
+        """
+        self.qptq_initialized = False
+        # Record matmul-view for GPTQ (K x C where C = product of remaining dims)
+        self.original_kernel_shape = kernel_shape
+        shape = list(self.original_kernel_shape)
+        try:
+            d_model_dim_index = shape.index(max(shape))
+        except ValueError:
+            raise TypeError(
+                f"Could not determine hidden dimension from shape {shape}"
+            )
+
+        if d_model_dim_index == 0:  # QKV projection case
+            in_features, heads, head_dim = shape
+            rows, columns = (
+                in_features,
+                heads * head_dim,
+            )
+        elif d_model_dim_index in [1, 2]:  # Attention Output case
+            heads, head_dim, out_features = shape
+            rows, columns = (
+                heads * head_dim,
+                out_features,
+            )
+        else:
+            raise ValueError("Could not determine row/column split.")
+
+        if config.group_size == -1:
+            n_groups = 1
+        else:
+            n_groups = ceil(rows / config.group_size)
+
+        # Mark that we've switched to the quantized (int8) path
+        if hasattr(self, "_set_quantization_info"):
+            self._set_quantization_info()
+
+        # INT8 kernel stored in the ORIGINAL N-D kernel shape
+        self.quantized_kernel = self.add_weight(
+            name="kernel",
+            shape=(rows, columns,),
+            initializer="zeros",
+            dtype="int32",
+            trainable=False,
+        )
+
+        # Scales/zeros have the EXACT SAME SHAPE as kernel (no reductions).
+        # Forward does: W = (q - zero_point) * kernel_scale
+        self.kernel_scale = self.add_weight(
+            name="kernel_scale",
+            shape=(columns, n_groups),
+            initializer="ones",
+            dtype="float32",
+            trainable=False,
+        )
+        self.kernel_zero = self.add_weight(
+            name="zero_point",
+            shape=(columns, n_groups),
+            initializer="zeros",
+            dtype="float32",
+            trainable=False,
+        )
+
+        self.g_idx = self.add_weight(
+            name="g_idx",
+            shape=(rows,),
+            initializer="zeros",
+            dtype="int32",
+            trainable=False,
+        )
+
+    def _gptq_call(self, inputs, training=False):
+        # TODO: This is getting called during calibration, which means the calibration
+        # is happening using the uninitialized quantized_kernel. We need to somehow make
+        # sure that the regular call method is called during that time.
+        if not self._is_quantized:
+            return self.call(inputs, training=training)
+        # q = ops.cast(self.quantized_kernel, "float32")
+        W = dequantize_with_sz_map(
+            self.quantized_kernel,
+            self.kernel_scale,
+            self.kernel_zero,
+            self.g_idx,
+        )
+        W = ops.transpose(W)
+
+        W = ops.reshape(W, self.original_kernel_shape)
+
+        y = ops.einsum(self.equation, inputs, W)
+        if getattr(self, "use_bias", False):
+            y = ops.add(y, self.bias)
+        return y
+
     def _int4_build(self, kernel_shape):
         """Build variables for int4 quantization.
 
@@ -756,13 +862,13 @@ class EinsumDense(Layer):
             x = self.activation(x)
         return x
 
-    def quantize(self, mode, type_check=True):
+    def quantize(self, mode, type_check=True, config=None):
         # Prevent quantization of the subclasses
         if type_check and (type(self) is not EinsumDense):
             raise self._not_implemented_error(self.quantize)
 
         kernel_shape = self._kernel.shape
-        if mode in ("int8", "int4"):
+        if mode in ("int8", "int4", "gptq"):
             self._set_quantization_info()
 
         if mode == "int8":
@@ -790,7 +896,7 @@ class EinsumDense(Layer):
             )
             kernel_value = packed_kernel_value
             del self._kernel
-        self.quantized_build(kernel_shape, mode)
+        self.quantized_build(kernel_shape, mode, config=config)
 
         # Assign values to the newly created variables.
         if mode in ("int8", "int4"):
@@ -800,7 +906,11 @@ class EinsumDense(Layer):
         # Set new dtype policy
         if self.dtype_policy.quantization_mode is None:
             policy = dtype_policies.get(f"{mode}_from_{self.dtype_policy.name}")
+            if mode == "gptq":
+                self._is_quantized = True
             self.dtype_policy = policy
+            if mode == "gptq":
+                self._is_quantized = False
 
     def _get_kernel_scale_shape(self, kernel_shape):
         """Get the shape of the kernel scale tensor.
diff --git a/keras/src/layers/core/embedding.py b/keras/src/layers/core/embedding.py
index e9a207daf..2953b8997 100644
--- a/keras/src/layers/core/embedding.py
+++ b/keras/src/layers/core/embedding.py
@@ -363,7 +363,7 @@ class Embedding(Layer):
             )
         return outputs
 
-    def quantize(self, mode, type_check=True):
+    def quantize(self, mode, type_check=True, config=None):
         # Prevent quantization of the subclasses
         if type_check and (type(self) is not Embedding):
             raise self._not_implemented_error(self.quantize)
diff --git a/keras/src/layers/layer.py b/keras/src/layers/layer.py
index 3c4ae8ab0..11e4046c7 100644
--- a/keras/src/layers/layer.py
+++ b/keras/src/layers/layer.py
@@ -1268,7 +1268,7 @@ class Layer(BackendLayer, Operation):
     def quantized_build(self, input_shape, mode):
         raise self._not_implemented_error(self.quantized_build)
 
-    def quantize(self, mode, type_check=True):
+    def quantize(self, mode, type_check=True, config=None):
         raise self._not_implemented_error(self.quantize)
 
     def _check_quantize_args(self, mode, compute_dtype):
@@ -1318,6 +1318,8 @@ class Layer(BackendLayer, Operation):
             return self._float8_call(*args, **kwargs)
         elif self.quantization_mode == "int4":
             return self._int4_call(*args, **kwargs)
+        elif self.quantization_mode == "gptq":
+            return self._gptq_call(*args, **kwargs)
         else:
             raise self._quantization_mode_error(self.quantization_mode)
 
@@ -1330,6 +1332,9 @@ class Layer(BackendLayer, Operation):
     def _float8_call(self, *args, **kwargs):
         raise self._not_implemented_error(self._float8_call)
 
+    def _gptq_call(self, *args, **kwargs):
+        raise self._not_implemented_error(self._gptq_call)
+
     def _not_implemented_error(self, attr, msg=None):
         if callable(attr):
             attr_name = attr.__name__
diff --git a/keras/src/models/model.py b/keras/src/models/model.py
index f519e26c1..41cb3ba12 100644
--- a/keras/src/models/model.py
+++ b/keras/src/models/model.py
@@ -435,17 +435,8 @@ class Model(Trainer, base_trainer.Trainer, Layer):
         """
         from keras.src.dtype_policies import QUANTIZATION_MODES
 
-        if mode == "gptq":
-            if not isinstance(config, GPTQConfig):
-                raise ValueError(
-                    "The `config` argument must be of type "
-                    "`keras.quantizers.GPTQConfig`."
-                )
-            gptq_quantize(self, config)
-            return
-
         # For all other modes, verify that a config object was not passed.
-        if config is not None:
+        if config is not None and mode != "gptq":
             raise ValueError(
                 f"The `config` argument is only supported for 'gptq' mode, "
                 f"but received mode='{mode}'."
@@ -467,12 +458,21 @@ class Model(Trainer, base_trainer.Trainer, Layer):
             list_of_sublayers = list(layer._flatten_layers())
             if len(list_of_sublayers) == 1:  # leaves of the model
                 try:
-                    layer.quantize(mode, type_check=type_check)
+                    layer.quantize(mode, type_check=type_check, config=config)
                     mode_changed = True
                 except NotImplementedError as e:
                     warnings.warn(str(e))
         # We need to set these functions to `None` to remake them for changed
         # call function
+        if mode == "gptq":
+            if not isinstance(config, GPTQConfig):
+                raise ValueError(
+                    "The `config` argument must be of type "
+                    "`keras.quantizers.GPTQConfig`."
+                )
+            gptq_quantize(self, config)
+            # return
+
         if mode_changed:
             self.train_function = None
             self.test_function = None
diff --git a/keras/src/quantizers/gptq.py b/keras/src/quantizers/gptq.py
index 987aa265f..a1fa0b906 100644
--- a/keras/src/quantizers/gptq.py
+++ b/keras/src/quantizers/gptq.py
@@ -7,7 +7,9 @@ from keras.src.ops import linalg
 from keras.src.quantizers.gptq_config import GPTQConfig
 from keras.src.quantizers.quantizers import GPTQQuantizer
 from keras.src.quantizers.quantizers import compute_quantization_parameters
+from keras.src.quantizers.quantizers import dequantize_with_sz_map
 from keras.src.quantizers.quantizers import dequantize_with_zero_point
+from keras.src.quantizers.quantizers import quantize_with_sz_map
 from keras.src.quantizers.quantizers import quantize_with_zero_point
 
 
@@ -36,59 +38,32 @@ def gptq_quantize_matrix(
     compute_scale_zero=compute_quantization_parameters,
 ):
     """
-    Implements the GPTQ error correction updates.
-
-    For a single column update (column j):
-        e = invH[j, j] * (w_j - q_j)
-        W[:, j+1:] -= e * invH[j, j+1:]
-    where:
-    - w_j is the original column,
-    - q_j is the quantized column,
-    - invH is the inverse Hessian,
-    - e is the propagated error term.
-
-    Across entire blocks:
-        W[:, future] -= E_block * invH[block, future]
-    where:
-    - E_block is the quantization error accumulated for the current block,
-    - invH[block, future] denotes the cross-block slice of the inverse Hessian,
-    - W[:, future] are the columns yet to be quantized.
-
-    Args:
-        weights_transpose: Transposed weight matrix [out_features, in_features]
-         to quantize.
-        inv_hessian: Inverse Hessian matrix [in_features, in_features] for
-         error propagation.
-        blocksize: Size of the blocks to process (default: 128).
-        group_size: Size of the groups for parameter reuse
-         (default: -1, no grouping).
-        activation_order: Whether to apply activation-order permutation
-         (default: False).
-        order_metric: Metric for ordering features
-         (default: None, uses 1 / diag(invH)).
-        compute_scale_zero: Function to compute scale and zero for
-         quantization.
+    Same GPTQ error-correction procedure as before, but returns:
+      - scale: concatenated per-group scales  [out_features, n_groups]
+      - zero:  concatenated per-group zeros   [out_features, n_groups]
+      - g_idx: per-column group index         [in_features,]
+    Notes:
+      * We still *internally* quantize/dequantize columns to compute error
+        feedback (that’s how GPTQ updates future columns), but we no longer
+        build/return a simulated quantized weight matrix.
+      * When group_size == -1, a single group spans all columns.
+      * If activation_order=True, g_idx is mapped back to original column order.
     """
     in_features = ops.shape(weights_transpose)[1]
 
     # Optional activation-order permutation on feature axis (axis=1)
     if activation_order:
         if order_metric is None:
-            # Use 1 / diag(inverse_hessian) as importance proxy if H not
-            # available.
             order_metric = ops.reciprocal(
                 ops.add(ops.diagonal(inv_hessian), 1e-12)
             )
         else:
-            # sanitize provided metric
             order_metric = ops.cast(order_metric, "float32")
             order_metric = ops.where(
                 ops.isfinite(order_metric),
                 order_metric,
                 ops.zeros_like(order_metric),
             )
-
-        # Sort in descending order by importance
         perm = _stable_permutation(order_metric)
         inv_perm = ops.argsort(perm)
 
@@ -99,10 +74,21 @@ def gptq_quantize_matrix(
     else:
         perm = inv_perm = None
 
-    # weights_buffer: [out_features, in_features]
+    # Working buffers
     weights_buffer = weights_transpose
-    # quantized_weights_buffer: [out_features, in_features]
-    quantized_weights_buffer = ops.zeros_like(weights_buffer)
+
+    # For returning parameters
+    scale_chunks = []
+    zero_chunks = []
+
+    # We’ll build group indices after the loop
+    # (shape [in_features,], dtype int32)
+    # Compute "effective" group size
+    effective_group = ops.cond(
+        ops.equal(group_size, -1),
+        lambda: in_features,
+        lambda: group_size,
+    )
 
     # Process features in blocks
     for block_start in range(0, in_features, blocksize):
@@ -110,18 +96,13 @@ def gptq_quantize_matrix(
         block_size = block_end - block_start
 
         # Block views
-        # block_weights: [out_features, bsize]
         block_weights = weights_buffer[:, block_start:block_end]
-        # block_weights_quantized: [out_features, bsize]
-        block_weights_quantized = ops.zeros_like(block_weights)
-        # block_error: [out_features, bsize]
         block_error = ops.zeros_like(block_weights)
-        # block_inv_hessian: [bsize, bsize]
         block_inv_hessian = inv_hessian[
             block_start:block_end, block_start:block_end
         ]
 
-        # group cache for per-group s/z/maxq reuse
+        # Per-group cached params
         cached_scale = None
         cached_zero = None
         cached_maxq = None
@@ -129,58 +110,49 @@ def gptq_quantize_matrix(
 
         for block_idx in range(block_size):
             global_idx = block_start + block_idx
-            # weight_column: [out_features,]
             weight_column = block_weights[:, block_idx]
 
             # Group-wise parameter reuse (compute once per group)
-            if group_size != -1:
-                # Determine group boundaries
-                group_start = (global_idx // group_size) * group_size
+            if not ops.equal(effective_group, in_features):  # group_size != -1
+                group_start = (global_idx // effective_group) * effective_group
                 if group_start != cached_group_start:
-                    group_end = min(group_start + group_size, in_features)
-                    # group_slice: [out_features, group_len]
+                    group_end = min(group_start + effective_group, in_features)
                     group_slice = weights_buffer[:, group_start:group_end]
                     cached_scale, cached_zero, cached_maxq = compute_scale_zero(
-                        group_slice, weight=True
+                        group_slice
                     )
+                    # Store params once per group (in the order encountered)
+                    scale_chunks.append(cached_scale)
+                    zero_chunks.append(cached_zero)
                     cached_group_start = group_start
                 scale, zero, maxq = cached_scale, cached_zero, cached_maxq
             else:
-                # Per-column params
-                scale, zero, maxq = compute_scale_zero(
-                    ops.expand_dims(weight_column, 1), weight=True
-                )
+                # Single global group covering all columns
+                if cached_scale is None:
+                    cached_scale, cached_zero, cached_maxq = compute_scale_zero(
+                        weights_buffer
+                    )
+                    scale_chunks.append(cached_scale)
+                    zero_chunks.append(cached_zero)
+                    cached_group_start = 0
+                scale, zero, maxq = cached_scale, cached_zero, cached_maxq
 
-            # Quantize one column
-            # quantized_column: [out_features,]
-            quantized_column = quantize_with_zero_point(
+            # --- Internal quantize/dequantize for GPTQ error correction ---
+            q_col = quantize_with_zero_point(
                 ops.expand_dims(weight_column, 1), scale, zero, maxq
             )
-            quantized_column = dequantize_with_zero_point(
-                quantized_column, scale, zero
-            )[:, 0]
-            block_weights_quantized = ops.slice_update(
-                block_weights_quantized,
-                (0, block_idx),
-                ops.expand_dims(quantized_column, 1),
-            )
+            q_col = dequantize_with_zero_point(q_col, scale, zero)[:, 0]
 
             # Error feedback for remaining columns within the block
-            # diag: [out_features,]
-            diag = block_inv_hessian[block_idx, block_idx]
-            # error = (col - quantized_col) / block_inv_hessian[idx, idx]
-            # error: [out_features,]
-            error = ops.divide(
-                ops.subtract(weight_column, quantized_column), diag
-            )
-            # block_error: [out_features, bsize]
+            d = block_inv_hessian[block_idx, block_idx]
+            err = ops.divide(ops.subtract(weight_column, q_col), d)
             block_error = ops.slice_update(
-                block_error, (0, block_idx), ops.expand_dims(error, 1)
+                block_error, (0, block_idx), ops.expand_dims(err, 1)
             )
 
             if block_idx < block_size - 1:
                 update = ops.matmul(
-                    ops.expand_dims(error, 1),
+                    ops.expand_dims(err, 1),
                     ops.expand_dims(
                         block_inv_hessian[block_idx, block_idx + 1 :], 0
                     ),
@@ -192,18 +164,8 @@ def gptq_quantize_matrix(
                     ops.subtract(tail, update),
                 )
 
-        # Write block’s quantized columns into result
-        left = quantized_weights_buffer[:, :block_start]
-        right = quantized_weights_buffer[:, block_end:]
-        quantized_weights_buffer = ops.concatenate(
-            [left, block_weights_quantized, right], axis=1
-        )
-
         # Propagate block errors to *future* features (beyond the block)
         if block_end < in_features:
-            # weights_buffer[:, block_end:] -=
-            # block_error @ invH[block_start:block_end, block_end:]
-            # total_update: [out_features, bsize]
             total_update = ops.matmul(
                 block_error, inv_hessian[block_start:block_end, block_end:]
             )
@@ -215,13 +177,29 @@ def gptq_quantize_matrix(
                 axis=1,
             )
 
-    # Undo permutation if used
+    # Build group indices for each (possibly permuted) column
+    # base_group = effective_group (int)
+    base_group = effective_group
+    # g_idx in permuted domain
+    g_idx = ops.arange(0, in_features, dtype="int32")
+    g_idx = ops.divide(g_idx, base_group)  # integer division (floor)
+    g_idx = ops.cast(g_idx, "int32")
+
+    # Map group indices back to original column order if act-order was used
     if activation_order:
-        quantized_weights_buffer = ops.take(
-            quantized_weights_buffer, inv_perm, axis=1
-        )
+        g_idx = ops.take(g_idx, inv_perm, axis=0)
+
+    # Concatenate recorded group params
+    if len(scale_chunks) == 0:
+        # Edge case: no groups recorded (empty input); fall back to whole matrix
+        s, z, _ = compute_scale_zero(weights_transpose)
+        scale = s
+        zero = z
+    else:
+        scale = ops.concatenate(scale_chunks, axis=1)
+        zero = ops.concatenate(zero_chunks, axis=1)
 
-    return quantized_weights_buffer
+    return scale, zero, g_idx
 
 
 class GPTQ:
@@ -418,7 +396,7 @@ class GPTQ:
         # Compute the inverse Hessian, which is used for error correction
         inverse_hessian = linalg.inv(hessian_matrix)
 
-        quantized_weights = gptq_quantize_matrix(
+        scale, zero, g_idx = gptq_quantize_matrix(
             weights_matrix,
             inv_hessian=inverse_hessian,
             blocksize=blocksize,
@@ -428,15 +406,61 @@ class GPTQ:
             compute_scale_zero=self.quantizer.find_params,
         )
 
-        quantized_weights = ops.transpose(quantized_weights)
+        # ------- Determine maxq (integer range) -------
+        # Prefer a single global maxq derived from the quantizer configuration.
+        #
+        # If your GPTQConfig exposes `bits`, use that; otherwise, probe via
+        # find_params.
+        maxq = ops.cast(
+            ops.subtract(ops.power(2, self.config.weight_bits), 1), "float32"
+        )
+
+        # ------- Build quantized (dequantized-proxy) weight matrix -------
+        Q = quantize_with_sz_map(weights_matrix, scale, zero, g_idx, maxq)
 
-        if isinstance(self.original_layer, EinsumDense):
-            quantized_weights = ops.reshape(
-                quantized_weights, self.kernel_shape
-            )
+        # # ------- Restore original layer shape & assign -------
+        # Q = ops.transpose(Q)  # back to [in_features, out_features]
+
+        # if isinstance(self.original_layer, EinsumDense):
+        #     Q = ops.reshape(Q, self.kernel_shape)
 
         # Set the new quantized weights in the original layer
-        self.original_layer._kernel.assign(quantized_weights)
+        if ops.shape(Q) != ops.shape(self.original_layer.quantized_kernel):
+            raise ValueError(
+                f"Quantized weight shape {ops.shape(Q)} does not match "
+                f"original layer shape {ops.shape(self.original_layer.quantized_kernel)}."
+            )
+        if Q.dtype != self.original_layer.quantized_kernel.dtype:
+            raise ValueError(
+                f"Quantized weight dtype {Q.dtype} does not match "
+                f"original layer dtype {self.original_layer.quantized_kernel.dtype}."
+            )
+        if scale.shape != self.original_layer.kernel_scale.shape:
+            raise ValueError(
+                f"Scale shape {scale.shape} does not match "
+                f"original layer shape {self.original_layer.kernel_scale.shape}."
+            )
+        if scale.dtype != self.original_layer.kernel_scale.dtype:
+            raise ValueError(
+                f"Scale dtype {scale.dtype} does not match "
+                f"original layer dtype {self.original_layer.kernel_scale.dtype}."
+            )
+        if zero.shape != self.original_layer.kernel_zero.shape:
+            raise ValueError(
+                f"Zero shape {zero.shape} does not match "
+                f"original layer shape {self.original_layer.kernel_zero.shape}."
+            )
+        if zero.dtype != self.original_layer.kernel_zero.dtype:
+            raise ValueError(
+                f"Zero dtype {zero.dtype} does not match "
+                f"original layer dtype {self.original_layer.kernel_zero.dtype}."
+            )
+        self.original_layer.quantized_kernel.assign(Q)
+        # del self.original_layer._kernel
+        self.original_layer.kernel_scale.assign(scale)
+        self.original_layer.kernel_zero.assign(zero)
+        self.original_layer.g_idx.assign(g_idx)
+        self.original_layer._is_quantized = True
 
     def free(self):
         self.hessian = None
diff --git a/keras/src/quantizers/gptq_test.py b/keras/src/quantizers/gptq_test.py
index 4ac6b5d01..bd8006ab3 100644
--- a/keras/src/quantizers/gptq_test.py
+++ b/keras/src/quantizers/gptq_test.py
@@ -14,7 +14,9 @@ from keras.src.quantizers.gptq import GPTQ
 from keras.src.quantizers.gptq import _stable_permutation
 from keras.src.quantizers.gptq import gptq_quantize_matrix
 from keras.src.quantizers.gptq_config import GPTQConfig
+from keras.src.quantizers.quantizers import dequantize_with_sz_map
 from keras.src.quantizers.quantizers import dequantize_with_zero_point
+from keras.src.quantizers.quantizers import quantize_with_sz_map
 from keras.src.quantizers.quantizers import quantize_with_zero_point
 from keras.src.testing.test_utils import named_product
 
@@ -305,7 +307,7 @@ class GPTQTest(testing.TestCase):
         # there is no interaction between different features
         inverse_hessian = ops.eye(in_features, dtype="float32")
 
-        dequantized_weights = gptq_quantize_matrix(
+        scale_map, zero_map, g_idx = gptq_quantize_matrix(
             weights_transpose,
             inverse_hessian,
             blocksize=128,
@@ -314,6 +316,14 @@ class GPTQTest(testing.TestCase):
             compute_scale_zero=_compute_scale_zero,
         )
 
+        qmax = 15.0
+        quantized_weights = quantize_with_sz_map(
+            weights_transpose, scale_map, zero_map, g_idx, qmax
+        )
+        dequantized_weights = dequantize_with_sz_map(
+            quantized_weights, scale_map, zero_map, g_idx
+        )
+
         # Compare function output with columnwise direct application
         # of quantization.
         out = ops.zeros_like(weights_transpose)
@@ -468,6 +478,16 @@ CONFIGS = {
     "act_order": {"activation_order": True},
     "symmetric": {"symmetric": True},
     "group_wise": {"group_size": 2},
+    "group_wise_per_channel": {"group_size": 4, "per_channel": True},
+    "group_wise_act_order": {"group_size": 8, "activation_order": True},
+    "symmetric_act_order": {"symmetric": True, "activation_order": True},
+    "symmetric_per_channel": {"symmetric": True, "per_channel": True},
+    "all_together": {
+        "symmetric": True,
+        "per_channel": True,
+        "activation_order": True,
+        "group_size": 4,
+    },
 }
 
 
@@ -557,6 +577,7 @@ class TestModelQuantization(testing.TestCase):
 
         # Build classifier and tokenizer
         model = _get_sequence_classifier()
+        model.compile(jit_compile=False, run_eagerly=True)
         tokenizer = _char_tokenizer(vocab_size=VOCAB_SIZE, seq_len=SEQ_LEN)
 
         # Build an eval batch drawn from the SAME distribution as calibration
diff --git a/keras/src/quantizers/quantizers.py b/keras/src/quantizers/quantizers.py
index fef3c02aa..2d902efbf 100644
--- a/keras/src/quantizers/quantizers.py
+++ b/keras/src/quantizers/quantizers.py
@@ -667,7 +667,7 @@ class GPTQQuantizer(Quantizer):
         self.zero = None
         self.maxq = None
 
-    def find_params(self, input_tensor, weight=False):
+    def find_params(self, input_tensor, weight=True):
         """Finds quantization parameters (scale and zero) for a given tensor."""
         self.scale, self.zero, self.maxq = compute_quantization_parameters(
             input_tensor,
@@ -834,3 +834,75 @@ def dequantize_with_zero_point(input_tensor, scale, zero):
         KerasTensor. The dequantized tensor.
     """
     return ops.multiply(scale, ops.subtract(input_tensor, zero))
+
+
+def quantize_with_sz_map(weights_matrix, scale, zero, g_idx, maxq):
+    """Rebuild a dequantized-proxy weight matrix from group params.
+
+    Args:
+        weights_matrix: 2D tensor of shape [out_features, in_features].
+        scale: Per-group scale tensor of shape [out_features, n_groups].
+        zero: Per-group zero-point tensor of shape [out_features, n_groups].
+        g_idx: Integer tensor of shape [in_features,] mapping each column to
+            its group index.
+        maxq: Scalar (float) representing the maximum integer quantization
+            level (e.g., 2^bits - 1).
+
+    Returns:
+        A tensor `Q` with the same shape as `weights_matrix` containing the
+        dequantized (float) proxy weights produced by quantize->dequantize
+        with the provided group parameters.
+    """
+    in_features = ops.shape(weights_matrix)[1]
+    Q = ops.zeros_like(weights_matrix, dtype="int32")
+
+    # Column-wise reconstruction using per-group parameters
+    for j in range(in_features):
+        gid = g_idx[j]
+        # Select this column and its group's params
+        wj = weights_matrix[:, j : j + 1]  # [out_features, 1]
+        sj = scale[:, gid : gid + 1]  # [out_features, 1]
+        zj = zero[:, gid : gid + 1]  # [out_features, 1]
+
+        q_int = quantize_with_zero_point(wj, sj, zj, maxq)  # integer levels
+
+        # write back the dequantized (float) proxy
+        Q = ops.slice_update(Q, (0, j), ops.cast(q_int, "int32"))
+
+    return Q
+
+
+def dequantize_with_sz_map(weights_matrix, scale, zero, g_idx):
+    """Rebuild a dequantized-proxy weight matrix from group params.
+
+    Args:
+        weights_matrix: 2D tensor of shape [out_features, in_features].
+        scale: Per-group scale tensor of shape [out_features, n_groups].
+        zero: Per-group zero-point tensor of shape [out_features, n_groups].
+        g_idx: Integer tensor of shape [in_features,] mapping each column to
+            its group index.
+        maxq: Scalar (float) representing the maximum integer quantization
+            level (e.g., 2^bits - 1).
+
+    Returns:
+        A tensor `Q` with the same shape as `weights_matrix` containing the
+        dequantized (float) proxy weights produced by quantize->dequantize
+        with the provided group parameters.
+    """
+    in_features = ops.shape(weights_matrix)[1]
+    Q = ops.zeros_like(weights_matrix, dtype="float32")
+
+    # Column-wise reconstruction using per-group parameters
+    for j in range(in_features):
+        gid = g_idx[j]
+        # Select this column and its group's params
+        wj = weights_matrix[:, j : j + 1]  # [out_features, 1]
+        sj = scale[:, gid : gid + 1]  # [out_features, 1]
+        zj = zero[:, gid : gid + 1]  # [out_features, 1]
+
+        q_deq = dequantize_with_zero_point(wj, sj, zj)  # float proxy
+
+        # write back the dequantized (float) proxy
+        Q = ops.slice_update(Q, (0, j), q_deq)
+
+    return Q
